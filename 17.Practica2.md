# 1. Práctica #2 Full Demo <!-- omit in toc -->

> Escenario:

Se ha modernizado una aplicación monolítica separando sus componentes en microservicios, esta se compone de 4 aplicaciones que se comunican entre si.

Las imágenes de estos servicios ya fueron construidas y ahora tienen un requerimiento de infraestructura que soporte un sistema orquestado en contenedores.

Las aplicaciones se acceden por medio del dominio, donde USERLAB es el nombre de laboratorio asignado a cada estudiante, ejemplo: carlosxyz.kubelabs.com

A continuación los requerimientos específicos de cada uno de los módulos.

# 1. Configuracion General

## 1.1. Todos los objetos Kubernetes deben ser creados en archivos YAML, con el objetivo de recrear la solución de forma automatizada y con el mínimo esfuerzo.

## 1.2. Eliminar los INGRESS y otros recursos de laboratorios anteriores

## 1.3. Opcional: Instalar las extensiones de VSCode: YAML y Kubernetes.
## 1.4. Opcional: Utilizar Kubens para moverse entre Namespaces
## 1.5. Opcional: Crear una carpeta para los archivos yaml con el nombre de "proyecto-final"

## 1.6. Namespaces
Es requerido implementar dos grupos de objetos separados por los namespaces:
> public

> private

<details>
  <summary>Tips</summary>

  > [namespace](./assets/namespace-definition.yml)
</details>

# 2. Web Server

## 2.1. Debe ser creado en el namespace ***public***
## 2.2. Se debe acceder por el dominio USERLAB.kubelabs.dev (o .tk)

## 2.3. Es una página web reactiva que para este caso, debe ser accedida por HTTP (puerto 80) y su contenedor escucha por el puerto 8080.

## 2.4. Necesita una ruta apuntando a su root (/)

## 2.5. Para efectos de alta disponibilidad se requieren 2 replicas
## 2.6. Dirección de la imagen:
> cachac/kubelabs_webapp:1.1.7

## 2.7. La página requiere montar un archivo de configuración con las siguientes características:
### 2.7.1. Ubicación del archivo en la máquina virtual Bastion (máquina de trabajo):
> /home/kube/kubelabs/config.js

### 2.7.2. Editar el archivo config.js agregando el subdominio de cada estudiante
> Sustituir las XXXXXXX con el nombre del SUBDOMINIO y el TLD (.online ó .dev) según sea necesario

### 2.7.3. Dentro del POD montar el archivo en la ruta:
> /usr/share/nginx/html/config.js

<details>
  <summary>Tips</summary>


> [Deployment](./assets/deployment-definition.yml)
>
> [ingress](./assets/ingress-definition.yml)
>
> [Montar ConfigMap en Deployment Volumen (ver lineas 30 a 41)](./assets/deploy-configmap-nginx.yml)
>
> [ConfigMap](./assets/x/05.cm-webpage.yml)

</details>

# 3. Web Socket
## 3.1. Namespace ***public***

## 3.2. Este servicio se accede por la ruta websocket.USERLAB.kubelabs.com
## 3.3. La ruta es /graphql

## 3.4. Este es un servicio CRÍTICO que usa una variable de entorno para establecer la conexión con la página web:

> TOKEN_SECRET: PASS

## 3.5. Puerto 3001

## 3.6. Replicas: 1

## 3.7. Imagen:
> cachac/kubelabs_websocket:1.0.6

<details>
  <summary>Tips</summary>

> [secret](./assets/secret-definition.yml)
>
> [Pod secret linea 31](./assets/15/secret-demo-mysql.yml)
</details>

~~~~
								     				CHECK POINT #1
~~~~

# 4. Public API

## 4.1. Namespace ***public***

## 4.2. Este servicio se accede por la ruta api.USERLAB.kubelabs.com
## 4.3. La ruta es /graphql

## 4.4. Puerto 3000

## 4.5. Replicas: 2

## 4.6. Imagen:
> cachac/kubelabs_public_api:1.0.0

# 5. Private API

## 5.1. Namespace ***private***

## 5.2. Este servicio privado que no se publica por Internet. Se accede a él únicamente a través del public api, esta conexión se debe configurar en el punto 6.

## 5.3. Puerto 3002

## 5.4. Replicas: 1

## 5.5. Imagen:
> cachac/kubelabs_privateapi:1.0.2

# 6. Establecer comunicación entre API público y privado

## 6.1. El PUBLIC API utiliza una variable de entorno para establecer la conexión con el API privado:
> Crear la variable como configMap y con los siguientes datos:

Debe cambiar los valores del SERVICE y NAMESPACE en el link anterior

> PRIVATE_API: http://SERVICE.NAMESPACE.svc.cluster.local:3002/private


> Nota: /private, es la ruta del REST API, no se debe cambiar

## 6.2. Asignar el CM como variable de entorno en el PUBLIC API
<details>
  <summary>Tips</summary>

  > [configMap](./assets/configmap-definition.yml)
  >
	> [Asignar configMap al POD linea 22](./assets/deploy-configmap.yml)
</details>

~~~~
								     				CHECK POINT #2
~~~~
# 7. Recursos
## 7.1. Página web
> limits: CPU: 100m; memoria: 100Mi
>
> requests: cpu: 10m; memoria: 50Mi

## 7.2. Websocket
> limits: CPU: 250m; memoria: 200Mi
>
> requests: cpu: 100m; memoria: 100Mi

## 7.3. Public API
> limits: CPU: 200m; memoria: 200Mi
>
> requests: cpu: 100m; memoria: 100Mi

## 7.4. Private API
> limits: CPU: 200m; memoria: 200Mi
>
> requests: cpu: 100m; memoria: 100Mi

<details>
  <summary>Tips</summary>

  > [recursos](./assets/pod-range-dev.yml)
</details>

# 8. HealthChecks (Readiness/Liveness)

## 8.1. Websocket
> path: /healthcheck
>
>  puerto: 3081
>
> initialDelaySeconds: 10, periodSeconds: 30, timeoutSeconds: 5
## 8.2. Public API
> path: /healthcheck
>
>  puerto: 3080
>
> initialDelaySeconds: 10, periodSeconds: 30, timeoutSeconds: 5
## 8.3. Private API
> path: /healthcheck
>
>  puerto: 3082
>
> initialDelaySeconds: 10, periodSeconds: 30, timeoutSeconds: 5


<details>
  <summary>Tips</summary>

  > [liveness](./assets/pod-liveness.yml)
	> [readiness](./assets/pod-readiness.yml)
</details>


# 9. Estatégias de deployment

## 9.1. La página web debe ser desplegada perdiendo hasta el 50% de los pods
## 9.2. Las réplicas del websocket deben ser eliminadas en su totalidad y recreadas con cada despliegue

<details>
  <summary>Tips</summary>

  > [strategy RollingUpdate](./assets/deploy-lifecycle.yml)
</details>

~~~~
								     				CHECKPOINT FINAL
~~~~

# 10. Extras

## 10.1. Logs
Los logs generados por el PUBLIC API deben ser almacenados persistentemente con los siguientes valores:

- Namespace: public
- Storage Class STANDARD
- Escritura y lectura desde un único NODO (RWO)
- Almacenamiento 50Mi
- Ruta del logs en el POD: /app/logs

<details>
  <summary>Tips</summary>

> [storageClass - PVC](./assets/x/16.pvc-public-api.yml)
>
> [volume + volumeMounts lineas 53 a 60](./assets/x/10.public-api.yml)

</details>

## 10.2. HPA
> [info](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/)

> [extra info](https://www.kubecost.com/kubernetes-autoscaling/kubernetes-hpa/)

### 10.2.1. Activar Métricas
Validar si las métricas están instaladas
```
kubectl top pods -n public
```
> error: Metrics API not available

### 10.2.2. Si no está instalado el Metrics Server, ejecutar:
> [Instalacion Metrics Server](https://github.com/kubernetes-sigs/metrics-server?tab=readme-ov-file#installation)
```
kubectl apply -f /home/kube/kubelabs/assets/kind-hpa-components/components.yaml

# Comprobar el Pod de Metrics Server Running y Ready 1/1
k get pod -n kube-system -l k8s-app=metrics-server

# Comprobar métricas de pods
kubectl top pods -n public

# Comprobar métricas de nodos
kubectl top nodes
```
### 10.2.3. El API público es un servicio de alta demanda que debe crecer automaticamente cuando:
> el CPU sea superior al 35%

> la memoria sea superior al 50%

> minimo de replicas: 3

> máximo de replicas: 6
<details>
  <summary>Tips</summary>

  > [HPA Definition](./assets/x/17.hpa-public-api.yml)
</details>

### 10.2.4. Validar el HPA
```
k get hpa -n public
```
Resultado:
~~~~
NAME             REFERENCE                      TARGETS           MINPODS   MAXPODS   REPLICAS
hpa-public-api   Deployment/deploy-public-api   0%/35%, 28%/50%   3         6         3
~~~~
> Observar el % de los Targets: 0/35 (CPU) y 28/50 (MEMORIA)

### 10.2.5. Watch Metrics

```
# Watch Metrics Server:
watch kubectl top pod -n public

# Watch HPA
watch kubectl get hpa -n public
```

### 10.2.6. Load testing:
> Cambiar el -l app=public-api, por la etiqueta asignada a su deployment
```vim
# Exec al pod
kubectl exec -n public -it $(kubectl get pods -l app=public-api -o jsonpath="{.items[0].metadata.name}" -n public) -- sh

# Instalar stress-ng
apk add stress-ng

# ejecutar el stress al CPU por un minuto
stress-ng --matrix 1 -t 1m
```

### 10.2.7. Revisar los "Watch" con las métricas y cantidad de pods
> La cantidad de CPU incrementa

> La cantidad de Pods incrementa a 6


### 10.2.8. Otros Comandos
```
# Mem Usage:
kubectl exec -n public -it  <POD_NAME> -- cat /sys/fs/cgroup/memory/memory.usage_in_bytes | awk '{ foo = $1 / 1024 / 1024 ; print foo "MB" }'
#CPU Usage:
kubectl exec -n public -it  <POD_NAME> -- cat /sys/fs/cgroup/cpu/cpuacct.usage
```
> https://unix.stackexchange.com/questions/450748/calculating-cpu-usage-of-a-cgroup-over-a-period-of-time

> https://stackoverflow.com/questions/51641310/kubernetes-top-vs-linux-top

## 10.3. Opcional: investigar successThreshold & failureThreshold
> [docs](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes)
## 10.4. Opcional: investigar restart policy
> [docs](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy)
